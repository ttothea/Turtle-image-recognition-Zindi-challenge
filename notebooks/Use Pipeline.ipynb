{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3e6e27b1-188a-4afb-98f9-38818e508f23",
   "metadata": {},
   "source": [
    "# Image Modelling Part 2 - Use Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "159167b4-2c3d-4c4d-9f75-2cd5462277c6",
   "metadata": {},
   "source": [
    "In the first notebook we used shell commands to prepare and split our data into a train and evaluation set. \n",
    "Furthermore, we defined some functions that will allow us to directly import our pictures and the corresponding class labels and if we want to also augment our data. \n",
    "Now, we will import the functions from the `image_modelling.py` file and use them to facilitate the data preparation step in this notebook. \n",
    "Lastly, we will use Tensorflow and Keras to create and train our neuronal network to identify turtles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ac118d51-c16b-4c7f-a6bf-6276f7a7c0ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tobiasengbring/neuefische/Capstone_Project_Turtle_Recall/.venv/lib/python3.9/site-packages/jax/_src/lib/__init__.py:32: UserWarning: JAX on Mac ARM machines is experimental and minimally tested. Please see https://github.com/google/jax/issues/5501 in the event of problems.\n",
      "  warnings.warn(\"JAX on Mac ARM machines is experimental and minimally tested. \"\n"
     ]
    }
   ],
   "source": [
    "# Import required packages \n",
    "import tensorflow as tf\n",
    "import image_modeling   # import image_modeling.py file\n",
    "import tensorflow_hub as hub\n",
    "import datetime\n",
    "import csv\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "# Load the TensorBoard notebook extension\n",
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a387f387-ca7d-4679-a05e-f92de2e9177c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clear any logs from previous runs\n",
    "!rm -rf ./logs/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "559448f7-019f-4047-901c-b477cd27e1e6",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Check for Tensorflow version\n",
    "print(tf.__version__)\n",
    "tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "90b1bb25-2f7f-479e-9aeb-fd56f13b9284",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import variables from image_modelling.py file\n",
    "file = open(\"../data/train.csv\")\n",
    "reader = csv.reader(file)\n",
    "\n",
    "HEIGHT = image_modeling.HEIGHT\n",
    "WIDTH = image_modeling.WIDTH\n",
    "NCLASSES = image_modeling.NCLASSES\n",
    "CLASS_NAMES = image_modeling.CLASS_NAMES\n",
    "BATCH_SIZE = image_modeling.BATCH_SIZE\n",
    "TRAINING_SIZE = image_modeling.TRAINING_SIZE\n",
    "TRAINING_STEPS = (TRAINING_SIZE // BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5b00031-663c-4004-bf86-4b64e6cd6e5a",
   "metadata": {},
   "source": [
    "Double check if the variables now contain the correct values. ;) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07961c12-db7b-402e-adfa-5e55ae4cb061",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# You can compare this output with the variables in the image_modelling.py file...\n",
    "print(HEIGHT)\n",
    "print(CLASS_NAMES)\n",
    "print(NCLASSES)\n",
    "print(TRAINING_STEPS)\n",
    "print(TRAINING_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39bda7a2-6102-4eff-93b9-e9b74e6cd405",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "source": [
    "## Building our Model\n",
    "\n",
    "Building and training a neural network involves various steps: \n",
    "1. define the architecture of the model\n",
    "2. compile the model\n",
    "3. train the model\n",
    "4. evaluate the model\n",
    "\n",
    "We have to start with defining the architecture. Our neural network will consist of several layers that are chained together. The input layer of our model will take our input data and hand it over to the flatten layer, which is responsible for reformatting our data. It will transform the format of our images from a three-dimensional array (HEIGHT, WIDTH, 3) to a one-dimensional array of size HEIGHT * WIDTH * 3. \n",
    "After the pixels are flattened we use a dense layer that returns a logits array with length `NCLASSES`. Each node in this layer contains a score that indicates the current image belongs to one of the n classes. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caf568e1-d651-47ba-a15f-a4fa901170bc",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "source": [
    "### Simple Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e26c3f1e-f4cd-4f1e-9abb-4c49c2205ee9",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "'''\n",
    "# Lets create a simple linear model.\n",
    "def linear_model():\n",
    "    model = tf.keras.models.Sequential()\n",
    "    model.add(tf.keras.layers.InputLayer(input_shape=[HEIGHT, WIDTH, 3], name='image'))\n",
    "    model.add(tf.keras.layers.Flatten(data_format=\"channels_last\"))\n",
    "    # We want to have a simple linear model so we have \n",
    "    # no activation function. \n",
    "    model.add(tf.keras.layers.Dense(units=NCLASSES, activation=None))\n",
    "    return model\n",
    "    '''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6c74046-b4ed-42ec-9816-4ad3f68a74a4",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "source": [
    "Before we can train our model we need to compile it and define more settings. We have to choose a loss function, an optimizer and metrics. \n",
    "* The **loss function** measures how accurate the model is during training by calculating the model error. Usually we want to minimize this function to improve our model. As you can see in the [TensorFlow documentation](https://www.tensorflow.org/api_docs/python/tf/keras/losses) there are lot's of different loss functions to choose from. Some, e.g. the mean squared errror, hopefully look familiar to you. ;) \n",
    "* The **[optimizer](https://www.tensorflow.org/api_docs/python/tf/keras/optimizers)** defines how the model is updated based on the data and the loss function. One optimizer we've already covered earlier and which is also used for neural networks is the stochastic gradient descent (SGD) algorithm.   \n",
    "* The **metric** is used to monitor the training process. Here we can choose one of the metrics we've already encountered or [many more](https://www.tensorflow.org/api_docs/python/tf/keras/metrics). \n",
    "\n",
    "The following function compiles our model, loads the data using the `load_dataset()` function from the image_modelling.py file and trains the model on the loaded data. In the end the function returns our fitted model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4ea0e7a-60e1-405f-9b16-1fd512965815",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "'''\n",
    "def train_and_evaluate(model,batch_size=32):\n",
    "\n",
    "    model.compile(\n",
    "        optimizer=\"adam\", \n",
    "        # The model outputs one-hot-encoded logits, so we need\n",
    "        # use the sparse version of the crossentropy loss.\n",
    "        loss=tf.keras.losses.CategoricalCrossentropy(from_logits=True),\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    \n",
    "    \n",
    "    log_dir = \"logs/fit/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "    tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
    "    train_datagen, test_datagen = image_modeling.preprocess()\n",
    "    train_generator, validation_generator = image_modeling.use_image_generator(train_datagen, test_datagen, training=True)\n",
    "    \n",
    "    model.fit(\n",
    "        train_generator, \n",
    "        validation_data=validation_generator,\n",
    "        steps_per_epoch=TRAINING_STEPS, \n",
    "        epochs=10,\n",
    "        callbacks=[tensorboard_callback])\n",
    "          \n",
    "    return model\n",
    "    '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f2fdebe-9f42-4061-ac71-d5d4ad7f0d1d",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "''' # Build and train our model using the prior defined functions \n",
    "model = linear_model()\n",
    "trained_model = train_and_evaluate(model, BATCH_SIZE)\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56088167-b93d-4639-ac63-40255d0e6ca8",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "source": [
    "Let us use Tensorboard to monitor our results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90b04f0c-1e01-4539-8771-ecd56286261b",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "'''\n",
    "%tensorboard --logdir logs/fit\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7e8a69c-781d-41fc-8a4b-6f2d1b2de1bb",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "source": [
    "### Deep Neural Network\n",
    "\n",
    "Our simple model is not performing well. Maybe we can boost its performance by adding more layers.\n",
    "\n",
    "In the following `dnn_model()` function we add three more hidden, dense layers after the flatten layer to increase our models complexity. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce98cfb8-748e-43d4-bace-3af0733d2abe",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "'''\n",
    "# Lets compare a neural network with hidden layers to the linear model\n",
    "def dnn_model():\n",
    "    model = tf.keras.models.Sequential()\n",
    "    model.add(tf.keras.layers.InputLayer(input_shape=[HEIGHT, WIDTH, 3], name='image'))\n",
    "    model.add(tf.keras.layers.Flatten(data_format=\"channels_last\"))\n",
    "    model.add(tf.keras.layers.Dense(units = 40, activation = \"relu\"))\n",
    "    model.add(tf.keras.layers.Dense(units = 40, activation = \"relu\"))\n",
    "    model.add(tf.keras.layers.Dense(units = 30, activation = \"relu\"))\n",
    "    # We want to have a simple linear model so we have \n",
    "    # no activation function. \n",
    "    model.add(tf.keras.layers.Dense(units=NCLASSES, activation=None))\n",
    "    return model\n",
    "    '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97794117-9aee-4fda-a85c-4b513dd9575e",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "'''\n",
    "# Let us fit the deep neural network\n",
    "model = dnn_model()\n",
    "trained_model = train_and_evaluate(model, BATCH_SIZE)\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "383dc998-8c17-4195-9ea2-016cd783ce14",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "source": [
    "Adding more hidden layers to our model, did indeed increase the accuracy. But still, the model's performance leaves something to be desired. Since we are working with images, switching to a convolutional neural network might help."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80f44d66-bc7c-4ea5-9d40-0234126ba2f1",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "source": [
    "### Convolutional Neural Network\n",
    "\n",
    "CNN's are widely used for image recognition. They are regularized versions of DNN's able to be deeper without generating as much parameters due to its [convolutional](https://machinelearningmastery.com/convolutional-layers-for-deep-learning-neural-networks/) and pooling layers.\n",
    "\n",
    "The architecture of our CNN is even more complex. This time we combine dense layers with `Conv2D` and `MaxPooling2D` layers. The convolutional and max pooling layers are inserted between the input layer and the flatten layer. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db4670c8-ad93-4533-a1da-0c158e9f32c5",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "'''\n",
    "# Now let us move on to a CNN model. \n",
    "def cnn_model():\n",
    "    model = tf.keras.models.Sequential()\n",
    "    model.add(tf.keras.layers.InputLayer(input_shape=[HEIGHT, WIDTH, 3], name='image'))\n",
    "    model.add(tf.keras.layers.Conv2D(filters=10, kernel_size=[5, 5], padding=\"same\", activation=\"relu\"))\n",
    "    model.add(tf.keras.layers.MaxPooling2D(pool_size=[2, 2], strides=2))\n",
    "    model.add(tf.keras.layers.Conv2D(filters=20, kernel_size=[5, 5], padding=\"same\", activation=\"relu\"))\n",
    "    model.add(tf.keras.layers.MaxPooling2D(pool_size=[2, 2], strides=2))\n",
    "    model.add(tf.keras.layers.Flatten())\n",
    "    model.add(tf.keras.layers.Dense(units=300, activation=\"relu\"))\n",
    "    model.add(tf.keras.layers.Dense(units=NCLASSES, activation=None))\n",
    "    return model\n",
    "    '''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15784cd7-83e1-450d-a4c4-a1c2d8bfb64d",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "source": [
    "We can have a look at the architecture of our model with the method `.summary`. As you can see in the summary below, the output of each `Conv2D` and `MaxPooling2D` layer is also a three dimensional tensor of shape (height, width, channels). As we go deeper into the network the dimensions shrink. One advantage of the shrinking dimensions is that we can computationally afford to add more output channels in each convolutional layer. We can control the number of the output channels of those layers with the `filters` argument. \n",
    "\n",
    "However, at the end of our model we still need the combination of the flatten and dense layers to perform classification. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1231a24d-56dc-4019-aa5a-b0bde1e6f9f8",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "'''\n",
    "model = cnn_model()\n",
    "model.summary()\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e814fc79-a858-4c48-a582-0ea8996cb4a1",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "'''\n",
    "# Let us fit the convolutional neural network\n",
    "model = cnn_model()\n",
    "trained_model = train_and_evaluate(model, BATCH_SIZE)\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "783eea5c-83af-40f1-8410-aebdbdcaa013",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "source": [
    "We see the CNN give better results than the DNN. But we still have heavy overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfc42e33-a846-43dc-92bb-8a4fd3a5f125",
   "metadata": {},
   "source": [
    "## Transfer Learning\n",
    "\n",
    "Transfer learning is when a model is trained on one task and is then reused for another task. One approach to transfer learning is fine-tuning. Here you take a trained neural net, exchange the last layer (head) for another layer, that fits the new task and then train the weights of the last layer only. \n",
    "\n",
    "First we need to download the headless model (this can take a while) we use MobileNetV2 which is a CNN that was trained on the [ImageNet](https://en.wikipedia.org/wiki/ImageNet) dataset, consisting of over 14 million images:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc53795c-03bb-4aae-955c-2ecd4ce3ddd5",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "'''\n",
    "feature_extractor_url = \"https://tfhub.dev/google/tf2-preview/mobilenet_v2/feature_vector/2\"\n",
    "feature_extractor_layer = hub.KerasLayer(feature_extractor_url,input_shape=(HEIGHT,WIDTH,3))\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4d37d23-a8c8-481c-bc07-b24dec923217",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "source": [
    "We only want to train the last layer therefore we freeze the layers of our headless model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cd5b0b4-53c8-4d0f-9ea0-a3b2fff80779",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "'''\n",
    "feature_extractor_layer.trainable = False\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e098ce17-6fe8-4f30-a378-5961b7e714e6",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "source": [
    "Now we define our model by simply adding the output layer to our pretrained net."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c93b6e1-e8ce-4202-8351-94be03fbda34",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "'''\n",
    "def transfer_learning_model():\n",
    "    model = tf.keras.models.Sequential()\n",
    "    model.add(feature_extractor_layer)\n",
    "    # TODO: add the correct output layer here\n",
    "    model.add(tf.keras.layers.Flatten())\n",
    "    model.add(tf.keras.layers.Dense(units=300, activation=\"relu\"))\n",
    "    model.add(tf.keras.layers.Dense(units=NCLASSES, activation=None))\n",
    "    return model\n",
    "    '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8211eae9-746e-4f39-a10b-d4d802538955",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "'''\n",
    "# Let us fit our transfer learning model\n",
    "model = transfer_learning_model()\n",
    "trained_model = train_and_evaluate(model, BATCH_SIZE)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2b3232d-a28c-411b-bcb3-afe050c5a7f8",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "'''\n",
    "%tensorboard --logdir logs/fit\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cde9b069-d2b8-4a31-bc4c-3b66550b9479",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "source": [
    "As we see the results of fine-tuning surpass the results of the linear model, DNN and CNN. Fine-tuning is a very powerful approach which can generalize well even with limited amount of data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05e71867-08a1-4e3c-bc39-601f6d8bbff1",
   "metadata": {},
   "source": [
    "### Transfer Learning InceptionV3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3caebcfe-6dfa-4006-8ec0-18534b1f3748",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.applications.inception_v3 import InceptionV3\n",
    "base_model = InceptionV3(input_shape = (224, 224, 3), include_top = False, weights = 'imagenet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8d6c337c-9153-4cfb-98ac-c27094d4ecd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.optimizers import RMSprop\n",
    "from tensorflow.keras import datasets, layers, models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "327924b2-bf68-45d4-898d-add3a6c9ed22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metal device set to: Apple M1\n"
     ]
    }
   ],
   "source": [
    "loaded_model = tf.keras.models.load_model('extra_images_location')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4f949222-5f28-4d0b-a151-72d4ba7882ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1502 validated image filenames belonging to 101 classes.\n",
      "Found 643 validated image filenames belonging to 101 classes.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-02-11 08:21:51.897435: W tensorflow/core/platform/profile_utils/cpu_utils.cc:128] Failed to get CPU frequency: 0 Hz\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "31/31 [==============================] - 58s 2s/step - loss: 0.4952 - top_k_categorical_accuracy: 0.0424 - val_loss: 0.2139 - val_top_k_categorical_accuracy: 0.0731\n",
      "Epoch 2/10\n",
      "31/31 [==============================] - 53s 2s/step - loss: 0.0969 - top_k_categorical_accuracy: 0.0808 - val_loss: 0.0561 - val_top_k_categorical_accuracy: 0.1073\n",
      "Epoch 3/10\n",
      "31/31 [==============================] - 54s 2s/step - loss: 0.0548 - top_k_categorical_accuracy: 0.1495 - val_loss: 0.0547 - val_top_k_categorical_accuracy: 0.1198\n",
      "Epoch 4/10\n",
      "31/31 [==============================] - 53s 2s/step - loss: 0.0541 - top_k_categorical_accuracy: 0.1566 - val_loss: 0.0545 - val_top_k_categorical_accuracy: 0.1213\n",
      "Epoch 5/10\n",
      "31/31 [==============================] - 53s 2s/step - loss: 0.0538 - top_k_categorical_accuracy: 0.1485 - val_loss: 0.0545 - val_top_k_categorical_accuracy: 0.1213\n",
      "Epoch 6/10\n",
      "31/31 [==============================] - 53s 2s/step - loss: 0.0539 - top_k_categorical_accuracy: 0.1475 - val_loss: 0.0545 - val_top_k_categorical_accuracy: 0.1198\n",
      "Epoch 7/10\n",
      "31/31 [==============================] - 53s 2s/step - loss: 0.0537 - top_k_categorical_accuracy: 0.1556 - val_loss: 0.0546 - val_top_k_categorical_accuracy: 0.1166\n",
      "Epoch 8/10\n",
      "31/31 [==============================] - 53s 2s/step - loss: 0.0538 - top_k_categorical_accuracy: 0.1455 - val_loss: 0.0546 - val_top_k_categorical_accuracy: 0.1198\n",
      "Epoch 9/10\n",
      "31/31 [==============================] - 53s 2s/step - loss: 0.0537 - top_k_categorical_accuracy: 0.1562 - val_loss: 0.0546 - val_top_k_categorical_accuracy: 0.1322\n",
      "Epoch 10/10\n",
      "31/31 [==============================] - 54s 2s/step - loss: 0.0538 - top_k_categorical_accuracy: 0.1404 - val_loss: 0.0546 - val_top_k_categorical_accuracy: 0.1275\n"
     ]
    }
   ],
   "source": [
    "#change the last layer\n",
    "for layer in loaded_model.layers:\n",
    "    layer.trainable = False\n",
    "\n",
    "x = layers.Flatten()(loaded_model.output)\n",
    "x = layers.Dense(1024, activation='relu')(x)\n",
    "x = layers.Dropout(0.2)(x)\n",
    "\n",
    "# Add a final softmax layer with 101 nodes for classification output\n",
    "x = layers.Dense(NCLASSES, activation='softmax')(x)\n",
    "\n",
    "model = tf.keras.models.Model(loaded_model.input, x)\n",
    "\n",
    "model.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = tf.keras.metrics.TopKCategoricalAccuracy(k=5))\n",
    "log_dir = \"logs/fit/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
    "train_datagen, test_datagen = image_modeling.preprocess()\n",
    "train_generator, validation_generator = image_modeling.use_image_generator(train_datagen, test_datagen, training=True)\n",
    "    \n",
    "inception =  model.fit(\n",
    "        train_generator, \n",
    "        validation_data=validation_generator,\n",
    "        steps_per_epoch=1000 // 32, \n",
    "        epochs=10,\n",
    "        callbacks=[tensorboard_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0fa7d27-ab89-405c-95f4-0cabe34ea0ff",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#%tensorboard --logdir logs/fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "192946b3-bbc8-4a7a-8631-dfa39b1fb724",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Quick and dirty: get a test_generator to predict probabilities\n",
    "\n",
    "test_datagen = image_modeling.ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "data=pd.read_csv('../data/test.csv')\n",
    "data.image_id= data.image_id.apply(lambda x: x.strip()+\".JPG\")\n",
    "train_dir=\"../images/\"\n",
    "\n",
    "test_generator = test_datagen.flow_from_dataframe(dataframe =data, \n",
    "                directory = train_dir,\n",
    "                x_col=\"image_id\",\n",
    "                target_size=(224, 224),\n",
    "                batch_size=1,\n",
    "                class_mode=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b78e7bdf-bcb6-40d8-a5f6-3ff26a16447b",
   "metadata": {},
   "source": [
    "## Prepare data for submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cfdbaaa0-ba92-4c7d-ab65-4330272f3207",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 490 validated image filenames.\n"
     ]
    }
   ],
   "source": [
    "test_generator = image_modeling.use_image_generator(train_datagen, test_datagen, training=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "485d4d3b-ca96-4479-bf18-6404d8daf8f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.00128775 0.0058658  0.00563102 0.01302973 0.00697664 0.00693395\n",
      "  0.008296   0.00747766 0.00781973 0.01353229 0.00598821 0.00461514\n",
      "  0.00706126 0.00800171 0.00912131 0.00387888 0.01591681 0.01159309\n",
      "  0.00883776 0.02409496 0.00588918 0.01520464 0.0074457  0.01139925\n",
      "  0.00715375 0.00617969 0.00442235 0.02169328 0.0072285  0.00437924\n",
      "  0.00427897 0.00724966 0.00452739 0.02273203 0.00407767 0.0134466\n",
      "  0.00457134 0.00906615 0.00494454 0.00625579 0.01225382 0.00614074\n",
      "  0.00580656 0.00288526 0.02266942 0.01036452 0.03349439 0.00508936\n",
      "  0.00535642 0.01045091 0.00895459 0.00480678 0.00792744 0.03061415\n",
      "  0.01048016 0.02405805 0.01191712 0.00565253 0.02785902 0.015731\n",
      "  0.00547631 0.0033218  0.01422122 0.00998242 0.01657759 0.01290275\n",
      "  0.0058864  0.02134658 0.00431259 0.00566628 0.00395388 0.01489166\n",
      "  0.01425694 0.00390338 0.00323622 0.00505687 0.00456915 0.03373691\n",
      "  0.00474063 0.00511677 0.0071164  0.00455384 0.00637645 0.00608567\n",
      "  0.00846918 0.01649156 0.00481377 0.00927037 0.01184476 0.00446472\n",
      "  0.0040716  0.00784734 0.0094633  0.00578835 0.01465115 0.02417081\n",
      "  0.01164767 0.01381799 0.00503442 0.00622867 0.01401805]\n",
      " [0.00120958 0.0049076  0.00506274 0.01231003 0.00719029 0.00614941\n",
      "  0.00798581 0.00830167 0.00693022 0.01244116 0.00619547 0.00427305\n",
      "  0.00686168 0.00723081 0.00991203 0.00397895 0.01735457 0.00991551\n",
      "  0.01165734 0.02152958 0.00713607 0.01292378 0.00823081 0.01169951\n",
      "  0.00638816 0.0057164  0.00522758 0.02016002 0.00677829 0.00392356\n",
      "  0.00438676 0.00647628 0.00477705 0.0257133  0.00474984 0.01490548\n",
      "  0.00440606 0.00793135 0.00595118 0.00591269 0.01154343 0.00682608\n",
      "  0.00614347 0.00275603 0.02313553 0.00910206 0.03686541 0.0050392\n",
      "  0.00503426 0.00896234 0.00987828 0.00450403 0.00726281 0.03496033\n",
      "  0.01194923 0.0222359  0.01098238 0.00526397 0.02777418 0.01689235\n",
      "  0.00485942 0.00327032 0.01411197 0.00949355 0.01872325 0.01144803\n",
      "  0.00472225 0.02503473 0.00375179 0.00530755 0.00370413 0.01494246\n",
      "  0.01603166 0.00360207 0.00273359 0.00440867 0.00480053 0.02967309\n",
      "  0.00564319 0.00535739 0.00815749 0.00387887 0.00512301 0.00664105\n",
      "  0.00889793 0.01414512 0.00483315 0.01280566 0.01202221 0.00445516\n",
      "  0.00419982 0.00776825 0.00847526 0.00511934 0.01342995 0.0268414\n",
      "  0.01325648 0.01217149 0.00517481 0.00549977 0.01358122]\n",
      " [0.00128775 0.0058658  0.00563102 0.01302973 0.00697664 0.00693395\n",
      "  0.008296   0.00747766 0.00781973 0.01353229 0.00598821 0.00461514\n",
      "  0.00706126 0.00800171 0.00912131 0.00387888 0.01591681 0.01159309\n",
      "  0.00883776 0.02409496 0.00588918 0.01520464 0.0074457  0.01139925\n",
      "  0.00715375 0.00617969 0.00442235 0.02169328 0.0072285  0.00437924\n",
      "  0.00427897 0.00724966 0.00452739 0.02273203 0.00407767 0.0134466\n",
      "  0.00457134 0.00906615 0.00494454 0.00625579 0.01225382 0.00614074\n",
      "  0.00580656 0.00288526 0.02266942 0.01036452 0.03349439 0.00508936\n",
      "  0.00535642 0.01045091 0.00895459 0.00480678 0.00792744 0.03061415\n",
      "  0.01048016 0.02405805 0.01191712 0.00565253 0.02785902 0.015731\n",
      "  0.00547631 0.0033218  0.01422122 0.00998242 0.01657759 0.01290275\n",
      "  0.0058864  0.02134658 0.00431259 0.00566628 0.00395388 0.01489166\n",
      "  0.01425694 0.00390338 0.00323622 0.00505687 0.00456915 0.03373691\n",
      "  0.00474063 0.00511677 0.0071164  0.00455384 0.00637645 0.00608567\n",
      "  0.00846918 0.01649156 0.00481377 0.00927037 0.01184476 0.00446472\n",
      "  0.0040716  0.00784734 0.0094633  0.00578835 0.01465115 0.02417081\n",
      "  0.01164767 0.01381799 0.00503442 0.00622867 0.01401805]]\n"
     ]
    }
   ],
   "source": [
    "#Get probabilities for all turtle id's\n",
    "y_preds = model.predict(test_generator)\n",
    "print(y_preds[0:3])\n",
    "#Get indices from top 5 predictions\n",
    "# Corrected: [:,:-6:-1] instead of [:,-5:]\n",
    "y_preds = np.argsort(y_preds, axis=1)[:,:-6:-1]\n",
    "\n",
    "#Save indices of top 5 predictions as dataframe\n",
    "df = pd.DataFrame(y_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e5b50197-f92b-4915-ac41-f3c549af0c08",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image_id</th>\n",
       "      <th>prediction1</th>\n",
       "      <th>prediction2</th>\n",
       "      <th>prediction3</th>\n",
       "      <th>prediction4</th>\n",
       "      <th>prediction5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ID_6NEDKOYZ</td>\n",
       "      <td>t_id_uMOOrQu7</td>\n",
       "      <td>t_id_smNwfXAT</td>\n",
       "      <td>t_id_IP1t15lD</td>\n",
       "      <td>t_id_HcnnlRda</td>\n",
       "      <td>t_id_dhdJMT1K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ID_57QZ4S9N</td>\n",
       "      <td>t_id_smNwfXAT</td>\n",
       "      <td>t_id_IP1t15lD</td>\n",
       "      <td>t_id_uMOOrQu7</td>\n",
       "      <td>t_id_HcnnlRda</td>\n",
       "      <td>t_id_dhdJMT1K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ID_OCGGJS5X</td>\n",
       "      <td>t_id_uMOOrQu7</td>\n",
       "      <td>t_id_smNwfXAT</td>\n",
       "      <td>t_id_IP1t15lD</td>\n",
       "      <td>t_id_HcnnlRda</td>\n",
       "      <td>t_id_dhdJMT1K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ID_R2993S3S</td>\n",
       "      <td>t_id_smNwfXAT</td>\n",
       "      <td>t_id_IP1t15lD</td>\n",
       "      <td>t_id_uMOOrQu7</td>\n",
       "      <td>t_id_HcnnlRda</td>\n",
       "      <td>t_id_dhdJMT1K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ID_2E011NB0</td>\n",
       "      <td>t_id_smNwfXAT</td>\n",
       "      <td>t_id_IP1t15lD</td>\n",
       "      <td>t_id_uMOOrQu7</td>\n",
       "      <td>t_id_HcnnlRda</td>\n",
       "      <td>t_id_dhdJMT1K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>485</th>\n",
       "      <td>ID_0RVNUKK1</td>\n",
       "      <td>t_id_uMOOrQu7</td>\n",
       "      <td>t_id_smNwfXAT</td>\n",
       "      <td>t_id_IP1t15lD</td>\n",
       "      <td>t_id_HcnnlRda</td>\n",
       "      <td>t_id_dhdJMT1K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>486</th>\n",
       "      <td>ID_6405IKG3</td>\n",
       "      <td>t_id_uMOOrQu7</td>\n",
       "      <td>t_id_smNwfXAT</td>\n",
       "      <td>t_id_IP1t15lD</td>\n",
       "      <td>t_id_HcnnlRda</td>\n",
       "      <td>t_id_dhdJMT1K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>487</th>\n",
       "      <td>ID_6WVPVB7S</td>\n",
       "      <td>t_id_uMOOrQu7</td>\n",
       "      <td>t_id_smNwfXAT</td>\n",
       "      <td>t_id_IP1t15lD</td>\n",
       "      <td>t_id_HcnnlRda</td>\n",
       "      <td>t_id_dhdJMT1K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>488</th>\n",
       "      <td>ID_47C5LL2G</td>\n",
       "      <td>t_id_uMOOrQu7</td>\n",
       "      <td>t_id_smNwfXAT</td>\n",
       "      <td>t_id_IP1t15lD</td>\n",
       "      <td>t_id_HcnnlRda</td>\n",
       "      <td>t_id_dhdJMT1K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>489</th>\n",
       "      <td>ID_HZP6EJAK</td>\n",
       "      <td>t_id_uMOOrQu7</td>\n",
       "      <td>t_id_smNwfXAT</td>\n",
       "      <td>t_id_IP1t15lD</td>\n",
       "      <td>t_id_HcnnlRda</td>\n",
       "      <td>t_id_dhdJMT1K</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>490 rows Ã— 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        image_id    prediction1    prediction2    prediction3    prediction4  \\\n",
       "0    ID_6NEDKOYZ  t_id_uMOOrQu7  t_id_smNwfXAT  t_id_IP1t15lD  t_id_HcnnlRda   \n",
       "1    ID_57QZ4S9N  t_id_smNwfXAT  t_id_IP1t15lD  t_id_uMOOrQu7  t_id_HcnnlRda   \n",
       "2    ID_OCGGJS5X  t_id_uMOOrQu7  t_id_smNwfXAT  t_id_IP1t15lD  t_id_HcnnlRda   \n",
       "3    ID_R2993S3S  t_id_smNwfXAT  t_id_IP1t15lD  t_id_uMOOrQu7  t_id_HcnnlRda   \n",
       "4    ID_2E011NB0  t_id_smNwfXAT  t_id_IP1t15lD  t_id_uMOOrQu7  t_id_HcnnlRda   \n",
       "..           ...            ...            ...            ...            ...   \n",
       "485  ID_0RVNUKK1  t_id_uMOOrQu7  t_id_smNwfXAT  t_id_IP1t15lD  t_id_HcnnlRda   \n",
       "486  ID_6405IKG3  t_id_uMOOrQu7  t_id_smNwfXAT  t_id_IP1t15lD  t_id_HcnnlRda   \n",
       "487  ID_6WVPVB7S  t_id_uMOOrQu7  t_id_smNwfXAT  t_id_IP1t15lD  t_id_HcnnlRda   \n",
       "488  ID_47C5LL2G  t_id_uMOOrQu7  t_id_smNwfXAT  t_id_IP1t15lD  t_id_HcnnlRda   \n",
       "489  ID_HZP6EJAK  t_id_uMOOrQu7  t_id_smNwfXAT  t_id_IP1t15lD  t_id_HcnnlRda   \n",
       "\n",
       "       prediction5  \n",
       "0    t_id_dhdJMT1K  \n",
       "1    t_id_dhdJMT1K  \n",
       "2    t_id_dhdJMT1K  \n",
       "3    t_id_dhdJMT1K  \n",
       "4    t_id_dhdJMT1K  \n",
       "..             ...  \n",
       "485  t_id_dhdJMT1K  \n",
       "486  t_id_dhdJMT1K  \n",
       "487  t_id_dhdJMT1K  \n",
       "488  t_id_dhdJMT1K  \n",
       "489  t_id_dhdJMT1K  \n",
       "\n",
       "[490 rows x 6 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Create a DataFrame with top 5 predictions in submission form\n",
    "list = []\n",
    "array = []\n",
    "for line in y_preds:\n",
    "    for id in line:\n",
    "        list.append(CLASS_NAMES[id])\n",
    "    array.append(list)\n",
    "    list = []\n",
    "\n",
    "titles = ['prediction1', 'prediction2','prediction3','prediction4','prediction5']\n",
    "\n",
    "submission = pd.DataFrame(array, columns= titles)\n",
    "\n",
    "#Insert image_ids from test_data\n",
    "test_data = pd.read_csv('../data/test.csv')\n",
    "submission.insert(loc=0, column='image_id', value=test_data['image_id'])\n",
    "submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "516f378f-7f5d-4310-a54e-5b153374ad1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save submission data as CSV\n",
    "submission.to_csv('../data/submission_extra_loc.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48c1d8bb-489b-4bf8-91d5-65da0ce4aeda",
   "metadata": {},
   "source": [
    "### Misc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa84066b-91ba-48b9-af7e-511cd9c0e0db",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save model\n",
    "'''\n",
    "model.save('InceptionV3')\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "155bff5d-fe2c-4000-a1c0-394b45a5a13f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load model\n",
    "'''\n",
    "loaded_model = tf.keras.models.load_model('InceptionV3)\n",
    "loaded_model.layers[0].input_shape #(None, 150, 150, 3)\n",
    "'''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
